# TIL_191105

date: Nov 05, 2019

### TIL_191105

---

- 표본으로 추적하는 확률변수
- PUBG 분석틀다지기

### 표본과 확률변수

---

본격적으로 데이터 분석과 관련된 내용을 공부하고 있다. 분명히 내가 알고 있는 개념들인데 뭔가 이질적인 기분이 너무 낯설었다. 내가 공부를 제대로 안한건가라는 생각이 들었지만 돌아와서 생각해보니 일반적으로 공부하는 방법과 다른 순서의 문제가 있었던 것 같다.

나는 대학교에서 확률론을 경제학을 기반으로 배웠다. 그래서 자연스럽게 경제학의 가상의 함수와 실제의 세계를 연결하는 순서가 경제학 개론적인 내용 (수요와 공급)에서 현실의 내용 (각종 데이터의 활용)의 순서로 배웠다. 이미 도메인 지식으로 확률변수에 대해서 이해를 먼저하고 그 다음에 실제 데이터인 표본을 다뤘다. 그래서 근본적인 질문인. 표본은 가상의 함수를 대표할 수 있는가 하는 문제에 크게 고민이 없었던 것 같다. 하지만 데이터 분석의 목표 상 샘플 데이터에서 알수 없고 뭉뜽그려진 어떤 함수를 찾기위해서 표본을 다룬다는 점이 작은 것들을 엮어서 큰 것을 추적한다는 개념에서 어긋남이 생긴 것 같다.

두번째는 용어가 헷갈려서 생기는 문제인데, 차라리 sample data라는 말이면 오히려 직관적으로 이해가 됐을 것 같지만 표본 공간이라는 낯선 단어와 벡터와 같은 소문자 기호들이 내 뇌 속에서 이질감으로 느껴졌던 것 같다. 직관적으로 소문자는 벡터→ 대문자는 메트릭스와 같은 관계라고 이해해서 그런 것도 있는 것 같다. 아마 그렇게 사용하기 위한 밑밥인 것 같긴하지만 기호의 낯섬과 이중성이 수학을 피곤하게 만드는 요소인 것 같기도 하다.

이 두 가지 이유로 오늘은 힘든 시간이었고 표본과 확률 변수의 함수를 찾기위한 여정이 쉽지 않다고 느껴졌다. 그래도 한달동안 기른 수학적 직관력과 나름의 배경지식 때문에 이해하는 것은 어렵지 않았다. 하지만 여전히 증명은 낯설긴하다. 그래도 다행인점은 한번 보고 논리적으로 증명을 따라하는 것은 아직까지는 어렵지 않은 것 같다. 오랜만에 어렵지 않은 느낌.

모멘트에대한 설명을 듣고 테일러 급수와 수학적인 차원의 표깃법에대한 내용이 떠올랐다. 테일러 급수는 함수를 모사하는 미분방정식의 중요한 공식 중 하나고 이게 표본의 모멘트에서 아마 사용될 것 같다. 2차원 분산, 3차원의 대칭성, 4차원의 밀집도 (단어가 생각이 안난다...) K차원 모멘트의 의미는 아마 함수의 복제성이라는 측면에서 테일러급수를 자연스럽게 연상하게 됐다. 수학적 상상력이 늘어난 것 같아서 개인적으로는 성취도를 느꼈던 부분이다. 또한, 어제 봤던 수학적 차원의 표깃법. 프렉탈의 차원문제 영상을 본 것도 내가 공부한 부분이 이렇게 쓰이겠구나 하는 뿌듯함이 생겼다.

[Taylor series | Essence of calculus, chapter 11](https://youtu.be/3d6DsjIBzJ4)

[Fractals are typically not self-similar](https://youtu.be/gB9n2gHsHN4)

> 이 동영상이 자연스럽게 떠올랐다는 점에서 나 스스로한테 대견했음

아무튼 주된 내용은 결국 샘플 데이터들을 어떻게하면 가상의 확률변수에대한 함수로 근사할 것인가가 목적인 것 같다. 그 과정에서 사용될 테크닉은 당연하게도 선형 대수학이고 이 선형 대수학을 사용하기 위한 독립변수의 활용과 그 의미들이 주된 내용일 것이다. 선형 대수학으로 풀 수 있는 독립변수들의 활용이 기본이고, 독립이 아니면 일어나게될 계산적인 문제들이 아마 여러가지로 발목을 잡을 것이라는 게 눈에보인다. 그렇기 때문에 이산확률분포에서 각 사건들이 독립적이기 때문에 계산이 가능하지만 차원이 높은 데이터를 다루는 데이터 분석에서는 일부는 포기하고 새로운 방법론으로 접근해야할 것 같다는 흐름이 눈에 그려진다.

돌아와서 오히려 집중해서 봐야할 것은 샘플데이터를 처리하는 방법이다. 지금은 평균이 근사의 왕인것 처럼 얘기하지만 여러가지 문제들 때문에 평균의 쓸모없음이 나올 것이고 이 쓸모없음을 극복하기 위해서 피똥싸는 노력이 필요할 것 같다. 아마 이 부분이 데이터의 전처리. 그러니까 계산이 가능하고 근사가 가능한 데이터로 만들고 근사가 가능해서 예측이 용이한 데이터로 만들기위한 피똥싸는 노력이 필요할 것이라고 생각된다.

아무튼 mean(x), E(X), E(mean(x)), Var(X) = 1/N(Var(x) 이 네가지 관계가 오늘 배운 가장 중요한 내용이 아닐까 싶다. 이제 여기에 응용으로 독립인 X, Y의 다변수 함수로의 활용도 배웠지만... 생각해보니 correlation-coeffection 에 대해서 조금 잘못 생각한 것 같기도하고... 아무튼 이쪽 부분은 마음을 리셋하고 공부를 해야할 것 같다.

### PUBG 분석틀 다지기

---

본격적인 EDA에 앞서서 간단하게 살펴본 데이터들로 내가 하려는 목적을 정리하면 좋을 것 같다. 내가 관심있는 문제는 PUBG의 현실적인 문제인 핵쟁이들과 각종 어뷰징, 게임의 목적인 승리와 무관한 이상치들을 분석하는 것이다.

분석 내용은 쥬피터 노트북으로 작성 → 한글로 캐글글을 올릴 예정이다.

기본적인 정리는 노션을 통해서 큰 틀을 잡고 각 주제에 맞는 아티클을 각 쥬피터 파일로 관리하는 것이 큰 목표이다.

오늘은 일단 도메인 지식을 늘리기 위해서 PUBG의 문제점과 전략들을 살펴봤다. 

1. 게임 모드 분리
    - 게임 모드는 가능하면 솔로큐를 중심으로 분석하고 이것이 스쿼드(솔로큐가 스쿼드에 포함될 경우도 필터링 해야한다.) 듀오로 승리 패턴의 양상이 어떻게 다를지 분석해야할 것 같다.
2. 데이터만으로 이상치를 필터링할 수 있을까?
    - 현실적으로는 어렵다고 생각한다. 다만 극단적인 이상치들을 제거할 계획이다.
    - 어떻게 제거할 것인가? : 이상치가 나온 해당 게임을 제거 or 이상치의 플레이어 제거
    - 핵을 제외하고 초반의 운의 영역 (아마 거의 최소 이동거리) 으로 플레이어가 겹쳐서 빠른 퇴장을 한 경우 어떻게 할 것인지... 고민해볼 문제이다.
    - 추가로 게임 모드별 이상치가 달라질 수 있다. (대표적으로 듀오, 스쿼드의 팀킬문제)
    - 앞서 살펴본 데이터 중 이상한 데이터가 있었다. (솔로인데 팀킬은 어떻게 가능한지...)
3. 승리패턴 분석
    - 일단 1등의 데이터 10분위 데이터를 분리할 것이다.
    - 1등의 전략과 10위권의 전략은 다를 것 같다.
    - 그리고 전략은 크게 우리의 관념상 자리잡은 간디메타와 여포메타. 그리고 특수 메타 중 하나였던 잠수메타를 분리할 것이다.
    - 데이터로 나올 수 있는 메타는 거리(존버, 뛰달), 데미지 (간디, 여포), 특수(잠수시간 등)으로 나뉠 것 같다.
4. 플레이어가 원하는 데이터 분석
    - opgg의 데이터들을 실제로 따라해보고 이걸 표현해보는 것
5. 플레이 모드별 비교
    - 앞선 데이터 셋을 전체 데이터와 부분데이터로 나누고 각 게임 모드별 샘플 데이터들의 차이를 비교해보면 좋을 것 같다.
    - 아마 이 부분이 시각화의 도움을 필요로하는 피날레 일 것 같다.

이정도 EDA 큰 그림을 그려봤다. 추가로 생각나는 아이디어는 그떄 그때 판다스를 통해서 직접 데이터를 만들어보는 것이 개인 프로젝트의 목표이다. 그래서 판다스와 친해지기가 이 데이터를 다루는 궁극적인 목표이다.

목적은 나의 판다스 실력향상이고 이 데이터 자체가 도구라는 메타적인 분석 방법이다.

앞으로의 일정관리는 조금 빡빡하게 해야할 것 같고, 수요일에는 일단 캐글에 올라온 의미있는 EDA들을 전부 살펴볼 예정이다. 생각보다 각 포인트에서 여러가지 아이디어들을 제공해줬기 때문에 판다스를 통해서 어떻게 데이터를 효율적으로 때었다 붙이지는지가 포인트 일 것 같다.

일단 오늘은 여기까지만 정리하기로 해야지. 근데 이것도 오늘 내가 배운 것에 포함되는지 모르겠다. ㅋ