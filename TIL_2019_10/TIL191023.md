### TIL191023
- 선형대수 복습
- PCA
- 뉴럴 네트워크 구조

### 선형대수 복습
지금까지 배웠던 선형 대수학을 복습했다.
선형 대수학의 공리의 조건들을 만족하면 무엇이든 선형 대수학의 개념을 활용할 수 있다. 즉 수의 체계가 아니라도 활용할 수 있는 멀티 툴이라는 얘기.
벡터의 방향에 대해서 다시 한번 복습했다. 열벡터와 행백터의 의미와 트랜포즈한 벡터와 열벡터를 구하면 내적이고 등등...
헷갈렸던 개념을 다시 바로 잡았다. 행렬의 놈과 벡터의 놈은 다르다. 일반적으로 벡터의 놈은 유클리드 거리로 측정하는데 행의 갯수가 3개 이상이 되는 벡터의 놈은 어떻게 계산되는지 좀 더 찾아봐야할 것 같다. 그 외에 노션으로 정리한 문서들 중에 빠진 것을 채우고 필요없는 부분들은 조금 지웠다.
고윳값과 대각화의 의미에 대해서 알게 됐다. (공)분산행렬을 구해서 연산을 쉽게 하고 각 고유벡터들은 좌표 평면상의 기저 벡터들과 같은 방향으로 움직이는 벡터들의 쌍이라는 것을 배웠다. 이 벡터들을 통해서 기저 벡터를 찾고 기저 벡터들을 통해서 자유롭게 행렬을 변화 시킬 수 있다는 것을 이해했다.
특잇값 분해에서 U 행렬은 어떤 기저 벡터들을 행렬의 쌍을 90도로 만들어 주는 역할을 하는 것을 알려주고 V^T 혹은 V^(-1) 행렬은 행렬을 U행렬과 쌍으로 기저들 새로운 좌표평면으로 옮겨주는 일종의 함수 역할을 하는 것을 배웠다. 

고유 벡터 $ AV=V \Rambda $
특잇값 $ A = U \Sigma V^(T) $
각 행렬의 공식들과 특잇값 분해의 유도도 살펴봤다.

각 분해의 목적은 좀 더 직관적인 이해를 위해서이고, 더 중요한 것은 구해진 \Sigma, eig 등을 통해서 행렬이 다른 차원에서 Scaling 한다는 것을 이해하는 것 같다.

뭐 시험은 대충 그냥  그랬지만 개념들은 내 머릿속에 있으니 도움이 되겠지 뭐

### PCA
좀 더 깊게 들어가기에는 너무 많은 부분들이 필요했고, '가장 영향을 많이 주는 데이터를 순서대로' 보여 줄 수 있는 방법 중 하나라는 것이 중요한 것 같다. 실제 차원이 낮아지는 곳으로 투영된 데이터가 어떤 w1, w2 등의 고유벡터들을 통해서 크기가 변화면서 고유벡터들의 각자의 의미를 생성한다는 점이 놀라웠다. 아직 깊이는 부족하지만 선형대수의 놀라운 활용성에 대해서 잘 알게 되었다.

### 뉴럴 네트워크
데이터의 도구적인 관점보다는 데이터를 다루기 위한 바탕을 깔고가는 시간이라고 생각하고 뉴럴 네트워크에 대해서도 어떤 알고리즘을 갖고 있는지 공부하기 시작했다. 여러가지 영상들도 보고 가능하면 영어로된 자료들도 읽어보려고 노력 중이다.

### 앞으로 할 일
가장 큰 고비가 지나갔으니 이번 주는 일단 기계적, 도구적으로 프로그래밍을 활용하는 법을 단련해야겠다. 내일은 아마 그런 시간을 갖을 것 같다. 내일 아침에는 프로그래밍 과제 좀 만져봐야겠다.
